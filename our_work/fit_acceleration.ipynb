{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c620d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import dynamiqs as dq\n",
    "\n",
    "class RhoProjector(nn.Module):\n",
    "    def forward(self, A):\n",
    "        A = A.to(torch.cfloat)\n",
    "        rho = 0.5 * (A + A.conj().transpose(-2, -1))\n",
    "        eigvals, eigvecs = torch.linalg.eigh(rho)\n",
    "        eigvals = torch.clamp(eigvals, min=0.0).to(torch.cfloat)\n",
    "        rho_psd = eigvecs @ torch.diag_embed(eigvals) @ eigvecs.conj().transpose(-2, -1)\n",
    "        trace = rho_psd.diagonal(dim1=-2, dim2=-1).sum(-1)\n",
    "        rho_psd = rho_psd / trace[..., None, None]\n",
    "        return rho_psd\n",
    "\n",
    "    \n",
    "class DummyModel(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(32 * 32, 2 * dim * dim)  # 2 for real+imag\n",
    "        self.dim = dim\n",
    "        self.projector = RhoProjector()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch = x.shape[0]\n",
    "        out = self.linear(x.view(batch, -1))\n",
    "        out = out.view(batch, 2, self.dim, self.dim)\n",
    "        A = out[:, 0] + 1j * out[:, 1]  # Complex-valued matrix\n",
    "        rho = self.projector(A)\n",
    "        return rho\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "508e3e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 16, 16])\n",
      "tensor(1.-4.4409e-16j, grad_fn=<TraceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dim = 16\n",
    "model = DummyModel(dim)\n",
    "x = torch.randn(8, 1, 32, 32)  # batch of 8 Wigner functions\n",
    "rho_pred = model(x)\n",
    "\n",
    "print(rho_pred.shape)          # (8, 16, 16)\n",
    "print(rho_pred[0].trace())     # Should be ~1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9f6baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CleanWignerDataset(Dataset):\n",
    "    def __init__(self, dim=40, size=32, n_samples=1000):\n",
    "        self.dim = dim\n",
    "        self.size = size\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        import numpy as np\n",
    "        import dynamiqs as dq\n",
    "\n",
    "        xvec = np.linspace(-4, 4, self.size)\n",
    "        yvec = np.linspace(-4, 4, self.size)\n",
    "\n",
    "        if np.random.rand() < 0.5:\n",
    "            n = np.random.randint(0, 10)\n",
    "            rho = dq.fock_dm(self.dim, n)\n",
    "        else:\n",
    "            real = np.random.uniform(1.0, 3.0)\n",
    "            imag = np.random.uniform(0.0, 0.3)\n",
    "            alpha = real + 1j * imag\n",
    "            rho = dq.coherent_dm(self.dim, alpha)\n",
    "\n",
    "        _, _, w_clean = dq.wigner(rho, xvec=xvec, yvec=yvec)\n",
    "\n",
    "        w_clean = torch.tensor(np.array(w_clean), dtype=torch.float32).unsqueeze(0)\n",
    "        rho = torch.tensor(np.array(rho), dtype=torch.complex64)\n",
    "\n",
    "        return w_clean, rho\n",
    "\n",
    "def fidelity_loss(rho_pred, rho_true, eps=1e-8):\n",
    "    # Force everything to complex\n",
    "    rho_pred = rho_pred.to(torch.cfloat)\n",
    "    rho_true = rho_true.to(torch.cfloat)\n",
    "\n",
    "    # Hermitize (important for numerical errors)\n",
    "    rho_pred = 0.5 * (rho_pred + rho_pred.conj().transpose(-2, -1))\n",
    "    rho_true = 0.5 * (rho_true + rho_true.conj().transpose(-2, -1))\n",
    "\n",
    "    # sqrt(rho_true)\n",
    "    eigvals, eigvecs = torch.linalg.eigh(rho_true)\n",
    "    eigvals = torch.clamp(eigvals, min=eps)\n",
    "    sqrt_rho = eigvecs @ torch.diag_embed(torch.sqrt(eigvals).to(torch.cfloat)) @ eigvecs.conj().transpose(-2, -1)\n",
    "\n",
    "    # inner product\n",
    "    inner = sqrt_rho @ rho_pred @ sqrt_rho\n",
    "\n",
    "    # sqrt of inner\n",
    "    eigvals_inner, eigvecs_inner = torch.linalg.eigh(inner)\n",
    "    eigvals_inner = torch.clamp(eigvals_inner, min=0.0)\n",
    "    sqrt_inner = eigvecs_inner @ torch.diag_embed(torch.sqrt(eigvals_inner).to(torch.cfloat)) @ eigvecs_inner.conj().transpose(-2, -1)\n",
    "\n",
    "    # Fidelity\n",
    "    fidelity = torch.real(sqrt_inner.diagonal(dim1=-2, dim2=-1).sum(-1)) ** 2\n",
    "    return 1 - fidelity.mean()\n",
    "\n",
    "def fidelity_proxy_loss(rho_pred, rho_true):\n",
    "    # Assume rho_pred and rho_true are complex Hermitian, trace-one\n",
    "    overlap = torch.real((rho_pred.conj() * rho_true).sum(dim=(-2, -1)))\n",
    "    return 1 - overlap.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab606fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# Full dataset\n",
    "full_dataset = CleanWignerDataset(dim=15, size=32, n_samples=10000)\n",
    "\n",
    "# 90% train, 10% val\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65c64778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, loss_fn, epochs=20, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for x, rho_true in train_loader:\n",
    "            x, rho_true = x.to(device), rho_true.to(device)\n",
    "            rho_pred = model(x)\n",
    "\n",
    "            loss = loss_fn(rho_pred, rho_true)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for x_val, rho_val in val_loader:\n",
    "                x_val, rho_val = x_val.to(device), rho_val.to(device)\n",
    "                rho_pred_val = model(x_val)\n",
    "                val_loss += loss_fn(rho_pred_val, rho_val).item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:02d} | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ecec3064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 0.022837 | Val Loss: 0.000151\n",
      "Epoch 02 | Train Loss: 0.000109 | Val Loss: 0.000109\n",
      "Epoch 03 | Train Loss: 0.000239 | Val Loss: 0.000057\n",
      "Epoch 04 | Train Loss: 0.000100 | Val Loss: 0.000014\n",
      "Epoch 05 | Train Loss: 0.000086 | Val Loss: 0.000126\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Choose loss:\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# loss_fn = frobenius_loss\u001b[39;00m\n\u001b[32m      6\u001b[39m loss_fn = fidelity_proxy_loss\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m train_model(model, train_loader, val_loader, optimizer, loss_fn, epochs=\u001b[32m50\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, loss_fn, epochs, device)\u001b[39m\n\u001b[32m      5\u001b[39m model.train()\n\u001b[32m      6\u001b[39m total_loss = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x, rho_true \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m      9\u001b[39m     x, rho_true = x.to(device), rho_true.to(device)\n\u001b[32m     10\u001b[39m     rho_pred = model(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ZenBook\\miniconda3\\envs\\alicebobhack\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28mself\u001b[39m._next_data()\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ZenBook\\miniconda3\\envs\\alicebobhack\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28mself\u001b[39m._dataset_fetcher.fetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ZenBook\\miniconda3\\envs\\alicebobhack\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ZenBook\\miniconda3\\envs\\alicebobhack\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m.dataset[\u001b[38;5;28mself\u001b[39m.indices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mCleanWignerDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     25\u001b[39m     imag = np.random.uniform(\u001b[32m0.0\u001b[39m, \u001b[32m0.3\u001b[39m)\n\u001b[32m     26\u001b[39m     alpha = real + \u001b[32m1\u001b[39mj * imag\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     rho = dq.coherent_dm(\u001b[38;5;28mself\u001b[39m.dim, alpha)\n\u001b[32m     29\u001b[39m _, _, w_clean = dq.wigner(rho, xvec=xvec, yvec=yvec)\n\u001b[32m     31\u001b[39m w_clean = torch.tensor(np.array(w_clean), dtype=torch.float32).unsqueeze(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ZenBook\\miniconda3\\envs\\alicebobhack\\Lib\\site-packages\\dynamiqs\\utils\\states.py:304\u001b[39m, in \u001b[36mcoherent_dm\u001b[39m\u001b[34m(dim, alpha)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcoherent_dm\u001b[39m(dim: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, ...], alpha: ArrayLike) -> QArray:\n\u001b[32m    265\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Returns the density matrix of a coherent state or a tensor product of coherent\u001b[39;00m\n\u001b[32m    266\u001b[39m \u001b[33;03m    states.\u001b[39;00m\n\u001b[32m    267\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    302\u001b[39m \u001b[33;03m        (2, 24, 24)\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m coherent(dim, alpha).todm()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ZenBook\\miniconda3\\envs\\alicebobhack\\Lib\\site-packages\\dynamiqs\\utils\\states.py:261\u001b[39m, in \u001b[36mcoherent\u001b[39m\u001b[34m(dim, alpha)\u001b[39m\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor(*[coherent(d, a) \u001b[38;5;28;01mfor\u001b[39;00m d, a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(dim, alpha, strict=\u001b[38;5;28;01mTrue\u001b[39;00m)])\n\u001b[32m    260\u001b[39m \u001b[38;5;66;03m# fact: dim is now an integer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m displace(\u001b[38;5;28mint\u001b[39m(dim), alpha) @ fock(\u001b[38;5;28mint\u001b[39m(dim), \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ZenBook\\miniconda3\\envs\\alicebobhack\\Lib\\site-packages\\dynamiqs\\utils\\operators.py:501\u001b[39m, in \u001b[36mdisplace\u001b[39m\u001b[34m(dim, alpha)\u001b[39m\n\u001b[32m    499\u001b[39m alpha = alpha[..., \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]  \u001b[38;5;66;03m# (..., 1, 1)\u001b[39;00m\n\u001b[32m    500\u001b[39m a = destroy(dim, layout=dense)  \u001b[38;5;66;03m# (n, n)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m501\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (alpha * a.dag() - alpha.conj() * a).expm()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ZenBook\\miniconda3\\envs\\alicebobhack\\Lib\\site-packages\\jax\\_src\\numpy\\array_methods.py:153\u001b[39m, in \u001b[36m_conj\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_conj\u001b[39m(\u001b[38;5;28mself\u001b[39m: Array) -> Array:\n\u001b[32m    149\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Return the complex conjugate of the array.\u001b[39;00m\n\u001b[32m    150\u001b[39m \n\u001b[32m    151\u001b[39m \u001b[33;03m  Refer to :func:`jax.numpy.conj` for the full documentation.\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m ufuncs.conj(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ZenBook\\miniconda3\\envs\\alicebobhack\\Lib\\site-packages\\jax\\_src\\numpy\\ufuncs.py:3281\u001b[39m, in \u001b[36mconj\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m   3278\u001b[39m \u001b[38;5;129m@export\u001b[39m\n\u001b[32m   3279\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconj\u001b[39m(x: ArrayLike, /) -> Array:\n\u001b[32m   3280\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Alias of :func:`jax.numpy.conjugate`\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3281\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m conjugate(x)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = DummyModel(dim=15)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Choose loss:\n",
    "# loss_fn = frobenius_loss\n",
    "loss_fn = fidelity_proxy_loss\n",
    "\n",
    "train_model(model, train_loader, val_loader, optimizer, loss_fn, epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a58197c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dynamiqs as dq\n",
    "\n",
    "dim = 15\n",
    "size = 32\n",
    "n = 3  # test Fock state |n⟩\n",
    "\n",
    "# --- Generate input ---\n",
    "xvec = np.linspace(-4, 4, size)\n",
    "yvec = np.linspace(-4, 4, size)\n",
    "rho_true = dq.fock_dm(dim, n)\n",
    "_, _, w_clean = dq.wigner(rho_true, xvec=xvec, yvec=yvec)\n",
    "\n",
    "# --- Prepare tensors ---\n",
    "w_tensor = torch.tensor(np.array(w_clean), dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # (1, 1, 32, 32)\n",
    "rho_true_tensor = torch.tensor(np.array(rho_true), dtype=torch.complex64).unsqueeze(0)     # (1, dim, dim)\n",
    "\n",
    "# --- Model prediction ---\n",
    "model.eval()\n",
    "model = model.to(\"cpu\")\n",
    "with torch.no_grad():\n",
    "    rho_pred = model(w_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af440d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted diagonal of rho:\n",
      "[3.9189155e-07 1.0800873e-06 7.5538242e-06 9.9997795e-01 2.7833298e-07\n",
      " 2.1053340e-06 1.1167782e-06 1.3070627e-06 9.5266267e-07 2.0959187e-06\n",
      " 9.7388443e-07 4.4181979e-07 6.1974487e-07 7.8401229e-07 2.3119180e-06]\n"
     ]
    }
   ],
   "source": [
    "# --- Extract and print diagonal ---\n",
    "diag = torch.real(torch.diagonal(rho_pred.squeeze(0)))\n",
    "print(\"Predicted diagonal of rho:\")\n",
    "print(diag.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3131cf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 11  # test Fock state |n⟩\n",
    "\n",
    "# --- Generate input ---\n",
    "xvec = np.linspace(-4, 4, size)\n",
    "yvec = np.linspace(-4, 4, size)\n",
    "rho_true = dq.fock_dm(dim, n)\n",
    "_, _, w_clean = dq.wigner(rho_true, xvec=xvec, yvec=yvec)\n",
    "\n",
    "# --- Prepare tensors ---\n",
    "w_tensor = torch.tensor(np.array(w_clean), dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # (1, 1, 32, 32)\n",
    "rho_true_tensor = torch.tensor(np.array(rho_true), dtype=torch.complex64).unsqueeze(0)     # (1, dim, dim)\n",
    "\n",
    "# --- Model prediction ---\n",
    "model.eval()\n",
    "model = model.to(\"cpu\")\n",
    "with torch.no_grad():\n",
    "    rho_pred = model(w_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ed341f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted diagonal of rho:\n",
      "[0.02247453 0.01118544 0.00531338 0.01118751 0.00202209 0.00276185\n",
      " 0.01937376 0.00459194 0.13395298 0.6242219  0.01238723 0.0163946\n",
      " 0.0811761  0.01389427 0.03906231]\n"
     ]
    }
   ],
   "source": [
    "# --- Extract and print diagonal ---\n",
    "diag = torch.real(torch.diagonal(rho_pred.squeeze(0)))\n",
    "print(\"Predicted diagonal of rho:\")\n",
    "print(diag.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c01dd3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alicebobhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
